---
layout:     post
title:      Python并发编程系列
subtitle:   多进程
date:       2019-10-24
author:     Walnut
header-img: img/post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - Python
---

# Python并发编程系列之多进程(multiprocess)

## 引言

本文主要对Python中并发编程中的多进程相关内容展开详细介绍，Python进程主要在multiprocess模块中，本文以multiprocess中Process类为中心，通过实例代码对多进程设计到的进程间的同步机制、通信机制、数据共享机制与进程池进行介绍。

## 创建进程

创建进程有两种方式，分别是通过定义函数的方式和通过定义类的方式。两种方式创建进程都必须通过实例化Process类。

Process类参数如下：
- group：这一参数值始终为None，尚未启用，是为以后Python版本准备的
- target：表示调用对象，即子进程要执行的任务
- args：表示调用对象的位置参数元组，即target的位置参数，必须是元组
- kwargs：表示调用对象的字典参数
- name：为子进程的名称

另外，无论用哪种方式创建进程都必须有
```if __name__ == '__main__':```
这一行代码作为程序入口，否则会报错。

### 通过定义函数的方式创建进程

通过函数方式创建进程这一方法需在实例化进程实例时将函数名作为参数传递进去，函数的参数用一个tuple传递给进程实例：

```python
from multiprocess import Process
import time

def func(n):
    print("子进程开始运行：{}......".format(n))
    time.sleep(1)
    print("子进程结束运行：{}......".format(n))

if __name__ == '__main__':
    print('主进程开始运行......')
    p = Process(target=func, args=(10,))
    p.start()
    time.sleep(1)

print("主进程结束运行......a")
```

### 通过类的方式创建进程

```python
from multiprocess import Process
import os

class MyProcess(Process):
    def run(self):
        print('子进程：', self.pid)

if __name__ == '__main__':
    p1 = MyProcess()
    p1.start()
    p2 = MyProcess()
    p2.start()

print('主进程：', os.getpid())
```

上面代码没有传递传输，如果要传递参数，那就要自定义构造方法，但在构造方法中，一定要先调用Process类的构造方法；

```python
from multiprocess import process
import os

class MyProcess(Process):
    def __init__(self, arg1, arg2):
        super().__init__()
        self.arg1 = arg1
        self.arg2 = arg2

    def run(self):
        print('子进程：', self.pid, self.arg1, self.arg2)

if __name__ == "__main__":
    p1 = MyProcess('I am arg1', 'I am arg2')
    p1.start()
    p2 = MyProcess('I am arg1', 'I am arg2')
    p2.start()

print('主进程：', os.getpid())
```

## Process的常见属性和方法

Process类常见属性如下

- daemon：默认值为False，如果设为True，则设为守护进程。
- name：进程的名称
- pid：进程的pid

Process类常用方法如下

- start()：启动进程，并调用该子进程中的p.run()
- run()：进程启动时运行的方法，正是它去调用target指定的函数，我们自定义类的类中一定要实现该方法
- terminate()：强制终止进程p，不会进行任何清理操作，如果p创建了子进程，该子进程就成了僵尸进程，使用该方法需要特别小心这种情况，如果p还保存了一个锁那么也将不会被释放，进而导致死锁
- is_alive()：如果p仍然运行，返回True
- join([timeout])：主进程等待子进程终止

### 守护进程：daemon

若要将一个进程设置为守护进程，在进程start之前，将其daemon属性设置为True即可。

```python
import time

import os

from multiprocess import Process

def func():

    i = 1

    while True:

        time.sleep(1)

        print('{}--子进程p1正在执行,pid:{}'.format(i , os.getpid()))

        i+=1

def func2():

    print('子进程p2开始执行,pid:{}'.format(os.getpid()))

    time.sleep(10)

    print('子进程p2结束执行,pid:{}'.format(os.getpid()))

if __name__=='__main__':

    print('主进程代码开始运行,pid:{}'.format(os.getpid()))

    p = Process(target=func)

    p.daemon = True # 设置为守护进程

    p.start()

    p2 = Process(target=func2)

    p2.start()

    time.sleep(5)

print('主进程代码运行完了,pid:{}'.format(os.getpid()))
```

得出结果，守护进程衣服于主进程代码，只要主进程代码运行完了，那么无论守护进程代码是否运行完，守护进程都会结束。另外，守护进程不能创建自己的子进程。

### 进程终结于存活检查：terminate()与is_alive()

terminate()与is_alive()都是由进程实例调用，分别用来终结一个进程、检查一个进程是否依然存活：

```python
import time

import os

from multiprocessing import Process

 

def func():

    i = 1

    while True:

        time.sleep(1)

        print('{}--子进程p1正在执行,pid:{}'.format(i , os.getpid()))

        i+=1


if __name__=='__main__':

    p = Process(target=func)

    p.start()

    time.sleep(3)

    p.terminate() # 终结进程p

    print(p.is_alive()) # 检查p是否依然存活

    time.sleep(1)

    print(p.is_alive())
```

为什么执行第一次terminate方法后is_alive方法输出为True，因为terminate()方法终结一个进程时操作系统需要一定的响应时间，所以可能会有延迟。

### join()方法

join方法功能是阻塞当前所在进程，等到被join的进程结束之后，回到当前进程继续执行。

```python
from multiprocess import Process

import time

def func1 ():

    print("进程1开始运行……")

    for i in range(3):

        time.sleep(1)

        print("进程1运行了{}秒……".format(i+1))

    print("进程1结束运行……")

 

def func2 ():

    print("进程2开始运行……")

    for i in range(6):

        time.sleep(1)

        print("进程2运行了{}秒……".format(i+1))

    print("进程2结束运行……")

 

if __name__ == '__main__':

    print("主进程开始运行……")

    p1 = Process(target=func1)

    p2 = Process(target=func2)

    p1.start()

    p2.start()

    time.sleep(1)

    p1.join()

    # p2.join()

    print("主进程结束运行……" )
```

上述代码不进行join、分别对进程1、进程2进行join的运行结果，发现，主进程会等待被join的进程运行完才会继续执行join下面的代码。

## 进程间的同步控制

### 进程锁：Lock

当多个进程对同一资源进行IO操作时，需要对资源“上锁”，否则会出现意外情况。上锁之后，同一资源就只能由一个进程运行上锁的代码块。

```python
from multiprocessing import Process

from multiprocessing import Lock

import time

import os

def func (lock):

    if os.path.exists('num.txt'):

        lock.acquire()

        with open('num.txt' , 'r') as f:

            num = int(f.read())

            num -= 1

        time.sleep(1)

        with open('num.txt' , 'w') as f:

            f.write(str(num))

        lock.release()

    else:

        with open('num.txt' , 'w') as f:

            f.write('10')

if __name__ == '__main__':

    print("主进程开始运行……")

    lock = Lock()

    p_list = []

    for i in range(10):

        p = Process(target=func , args=(lock,))

        p_list.append(p)

        p.start()

    for p in p_list:

        p.join()

    with open('num.txt', 'r') as f:

        num = int(f.read())

    print('最后结果为：{}'.format(num))

    print("主进程结束运行……" )
```

### 信号量：Semaphore

锁同时只允许一个线程更改数据，而信号量是同时允许一定数量的进程更改数据。

```python
from multiprocessing import Process

import time

import random

from multiprocessing import Semaphore

def fun(i , sem):

    sem.acquire()

    print('{}号顾客上座，开始吃饭'.format(i))

    time.sleep(random.randint(3, 8))

    print('{}号顾客吃完饭了，离座'.format(i))

    sem.release()


if __name__=='__main__':

    sem = Semaphore(3)

    for i in range(10):

        p = Process(target=fun, args=(i,sem))

        p.start()
```

事实上，Semaphore的作用也类似于锁，只不过在锁机制上添加了一个计数器，允许多个拥有“钥匙”

### 事件

python进程的事件用于主进程控制其他子进程的执行，Event类有如下几个主要方法：
- wait()：在进程中插入一个标记（flag）默认为False，当flag为False时，程序会停止运行并进入阻塞
- set()：使flag为True，程序会进入非阻塞状态
- clear()：使flag为False，程序会停止运行，进入阻塞状态
- is_set()：判断flag是否为True，是的话返回True，不是返回False


## 进程间通信

### 进程队列：Queue

常用方法：　　

- get( [ block [ ,timeout ] ] ) ：返回q中的一个项目。如果q为空，此方法将阻塞，直到队列中有项目可用为止。block用于控制阻塞行为，默认为True. 如果设置为False，将引发Queue.Empty异常（定义在Queue模块中）。timeout是可选超时时间，用在阻塞模式中。如果在制定的时间间隔内没有项目变为可用，将引发Queue.Empty异常。

- get_nowait( ) ：同get(False)方法。

- put(item [, block [,timeout ] ] ) ：将item放入队列。如果队列已满，此方法将阻塞至有空间可用为止。block控制阻塞行为，默认为True。如果设置为False，将引发Queue.Empty异常（定义在Queue库模块中）。timeout指定在阻塞模式中等待可用空间的时间长短。超时后将引发Queue.Full异常。

- qsize() ：返回队列中目前项目的正确数量。此函数的结果并不可靠，因为在返回结果和在稍后程序中使用结果之间，队列中可能添加或删除了项目。在某些系统上，此方法可能引发NotImplementedError异常。

- empty() ：如果调用此方法时队列为空，返回True。如果其他进程或线程正在往队列中添加项目，结果是不可靠的。也就是说，在返回和使用结果之间，队列中可能已经加入新的项目。

- full() ：如果q已满，返回为True. 由于线程的存在，结果也可能是不可靠的。

- close() ：关闭队列，防止队列中加入更多数据。调用此方法时，后台线程将继续写入那些已入队列但尚未写入的数据，但将在此方法完成时马上关闭。如果队列被垃圾收集，将自动调用此方法。关闭队列不会在队列使用者中生成任何类型的数据结束信号或异常。例如，如果某个使用者正被阻塞在get（）操作上，关闭生产者中的队列不会导致get（）方法返回错误。

- cancel_join_thread()：不会在进程退出时自动连接后台线程。这可以防止join_thread()方法阻塞。

- join_thread()：连接队列的后台线程。此方法用于在调用close()方法后，等待所有队列项被消耗。默认情况下，此方法由不是队列的原始创建者的所有进程调用。调用cancel_join_thread()方法可以禁止这种行为。

```python
from multiprocess import Process

from multiprocess import Queue

import random

import os

# 向queue中输入数据的函数

def inputQ(queue ):

    info = random.randint(1,100)

    queue.put(info)

    print('进程{}往队列中存了一个数据：{}'.format(os.getpid() , info))

# 向queue中输出数据的函数

def outputQ(queue):

    info = queue.get()

    print ('进程{}从队列中取出一个数据：{}'.format(os.getpid() , info))

if __name__ == '__main__':

    queue = Queue(5)

    lst_1  = []

    lst_2 = []

    for i in range(3):

        process = Process(target=inputQ,args=(queue,))

        process.start()

        lst_1.append(process)

    # 输出进程

    for i in range(2):

        process = Process(target=outputQ ,args=(queue,))

        process.start()

        lst_1.append(process)

    for p in lst_1:

        p.join()

    for p in lst_2:

        p.join()
```

### 管道(Pipe)

创建管道方法：

- Pipe([duplex]):在进程之间创建一条管道，并返回元组（conn1,conn2）,其中conn1，conn2表示管道两端的连接对象，强调一点：必须在产生Process对象之前产生管道。dumplex:默认管道是全双工的，如果将duplex射成False，conn1只能用于接收，conn2只能用于发送。

- conn1.recv():接收conn2.send(obj)发送的对象。如果没有消息可接收，recv方法会一直阻塞。如果连接的另外一端已经关闭，那么recv方法会抛出EOFError。

- conn1.send(obj):通过连接发送对象。obj是与序列化兼容的任意对象

- conn1.close():关闭连接。如果conn1被垃圾回收，将自动调用此方法

- conn1.fileno():返回连接使用的整数文件描述符

- conn1.poll([timeout]):如果连接上的数据可用，返回True。timeout指定等待的最长时限。如果省略此参数，方法将立即返回结果。如果将timeout射成None，操作将无限期地等待数据到达。

- conn1.recv_bytes([maxlength]):接收c.send_bytes()方法发送的一条完整的字节消息。maxlength指定要接收的最大字节数。如果进入的消息，超过了这个最大值，将引发IOError异常，并且在连接上无法进行进一步读取。如果连接的另外一端已经关闭，再也不存在任何数据，将引发EOFError异常。

- conn.send_bytes(buffer [, offset [, size]])：通过连接发送字节数据缓冲区，buffer是支持缓冲区接口的任意对象，offset是缓冲区中的字节偏移量，而size是要发送字节数。结果数据以单条消息的形式发出，然后调用c.recv_bytes()函数进行接收    

- conn1.recv_bytes_into(buffer [, offset]):接收一条完整的字节消息，并把它保存在buffer对象中，该对象支持可写入的缓冲区接口（即bytearray对象或类似的对象）。offset指定缓冲区中放置消息处的字节位移。返回值是收到的字节数。如果消息长度大于可用的缓冲区空间，将引发BufferTooShort异常

```python
from multiprocess import Process, Pipe

def f(conn):

    conn.send('主进程，你好呀！')  # 发送数据给主进程

    print('子进程收到主进程发来的数据：{}'.format(conn.recv()))

    conn.close()  # 关闭

if __name__ == '__main__':

    parent_conn, child_conn = Pipe()  #Pipe是一个函数，返回的是一个元组

    p = Process(target=f, args=(child_conn,))  # 创建一个子进程

    p.start()

    print("主进程收到子进程发来的数据：{}".format(parent_conn.recv()))

    parent_conn.send('子进程，你好啊！')  # 发送数据给子进程

p.join()

```

## 进程间数据共享

### Manager

进程间数据共享可以通过Manager

```python
import os

from multiprocess import Manager,Process

# 定义了一个foo函数,接收一个字典和一个列表

def foo(d, l):

    # 字典和列表都放进程ID

    d[os.getpid()] = os.getpid()

    l.append(os.getpid())

if __name__ == '__main__':

    # 生成Manager对象

    manager = Manager()

    d = manager.dict()

    l = manager.list(range(3))

    # 10个进程分别join

    p_list = []

    for i in range(10):

        p = Process(target=foo, args=(d, l))

        p.start()

        p_list.append(p)

    for res in p_list:

        res.join()
 
    # 打印字典和列表

    print(d)

    print(l)
```

### 进程池

为什么要用进程池呢？如果我们有几百上千个任务需要自行，那么按照之前的做法，我们就要创建几百上千个进程，每一个进程都要占用一定的内存空间，进程间的切换也费时，系统开销很大，而且，难道这成千上百个进程能同时并发执行的有几个呢？其实也就那么几个子，所以，根本没必要创建那么多进程。那么怎么办呢？那就创建进程池。进程池里有固定数量的进程，每次执行任务时都从进程池中取出一个空闲进程来执行，如果任务数量超过进程池中进程数量，那么就等待已经在执行的任务结束之后，有进程空闲之后再执行，也就是说，同一时间，只有固定数量的进程在执行，这样对操作系统得压力也不会太大，效率也得到保证。

- 同步执行

```python
import os,time,random

from multiprocess import Pool

def func1(n):

    print('任务{}开始执行，进程为：{}'.format(n,os.getpid()))

    time.sleep(random.randint(1,4))

    print('任务{}结束执行，进程为：{}'.format(n,os.getpid()))

if __name__ == '__main__':

    p=Pool(3) #c创建一个进程池，里面有三个进程

    for i in range(10):

        res=p.apply(func1,args=(i,))
```

- 异步执行

```python
import os,time,random
from multiprocess import Pool


def func1(n):
    print('任务{}开始执行，进程为：{}'.format(n,os.getpid()))

    time.sleep(random.randint(1,4))

    print('任务{}结束执行，进程为：{}'.format(n,os.getpid()))

if __name__ == '__main__':

    p=Pool(3) #c创建一个进程池，里面有三个进程

    for i in range(5):

        res=p.apply_async(func1,args=(i,))

    p.close()#一定要关闭

    p.join()#一定要使用join，不然进程池里的进程没来得及执行，主进程结束了，子进程也都跟着结束。
```

不加pool.close()，直接加pool.join()是会报错的，因为进程池里面的进程用完之后不会结束，而是被还到进程池了，因此这里的join检测的是没有任务再进入进程池了，而不是检测子进程的结束。所以要保证没有任务进入进程池，进程池就不会接收到任务，所以pool.close()的作用就是结束进程池接收任务，就是我的任务执行完毕，且没有新的任务进来，这是就被join检测到了。